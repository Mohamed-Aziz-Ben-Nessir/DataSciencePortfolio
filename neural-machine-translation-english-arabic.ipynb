{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Bidirectional, LSTM,  Dense, Softmax,Attention,Input,RepeatVector, Concatenate, Permute, Dot,Multiply,Activation\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the Data i am using a slightly modified ver from http://www.manythings.org\ndata=pd.read_csv(\"../input/englisharabic/eng-ara.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking for missing labels\ndata.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing(txt):\n    txt=\"\".join(c for c in txt if c not in string.punctuation).lower().strip()\n    txt.encode('utf8','ignore')\n    return txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in data.columns :\n    data[c]=data[c].apply(lambda x:text_preprocessing(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import unicodedata\n\nimport re\n\n# Convert the unicode sequence to ascii\ndef unicode_to_ascii(s):\n\n  # Normalize the unicode string and remove the non-spacking mark\n  return ''.join(c for c in unicodedata.normalize('NFD', s)\n      if unicodedata.category(c) != 'Mn')\ndef preprocess_sentence(w):\n\n# Preprocess the sequence\n  w = unicode_to_ascii(w.lower().strip())\n\n  # Create a space between word and the punctuation following it\n  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n  w = re.sub(r'[\" \"]+', \" \", w)\n\n  # Replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n  w = re.sub(r\"[^a-zA-Z?.!,¿\\u0600-\\u06FF]+\", \" \", w)\n\n  w = w.strip()\n\n  # Add a start and stop token to detect the start and end of the sequence\n  #w = '<start> ' + w + ' <end>'\n  return w","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in data.columns:\n    data[col]=data[col].apply(lambda x:preprocess_sentence(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp_txt = data[\"English\"].copy()\ntarg_txt= data[\"Arabic\"].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_targ_size=max_inp_size=15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp_tokenizer=Tokenizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp_tokenizer.fit_on_texts(inp_txt)\ninp_vocab=inp_tokenizer.word_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(inp_vocab)[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp_seq=inp_tokenizer.texts_to_sequences(inp_txt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp_seq=pad_sequences(inp_seq,maxlen=max_inp_size,padding=\"pre\",truncating='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(inp_seq)[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targ_tokenizer=Tokenizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targ_tokenizer.fit_on_texts(targ_txt)\ntarg_vocab=targ_tokenizer.word_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(targ_vocab)[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(list(targ_vocab))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inv_targ_vocab={v:k for k,v in targ_vocab.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(inv_targ_vocab)[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targ_seq=targ_tokenizer.texts_to_sequences(targ_txt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(targ_seq)[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targ_seq =pad_sequences(targ_seq,maxlen=max_targ_size,padding=\"pre\",truncating='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(targ_seq)[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oh_inp_seq=to_categorical(inp_seq,num_classes=len(inp_vocab)+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(oh_inp_seq)[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oh_targ_seq=to_categorical(targ_seq,num_classes=len(targ_vocab)+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(oh_targ_seq)[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the encoder,it's pre-attention bi diractional lstm \n#the number of units should be the size of the one-hot vector \n#the input shape consists of just one line(batch size 1) that contains 15 words (padding included)\n#the return_sequence should be set to true to get the respetive hidden state of each input \nencoder = Bidirectional(LSTM(units=4040,return_sequences=True,input_shape=(1, 15,8080 )))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the decoder, the post attention lstm\n#the return_sequence should be set to true to get the respetive hidden state of each input \ndecoder = LSTM(8080, return_state = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.backend as K\ndef softmax(x, axis=1):\n    \"\"\"Softmax activation function.\n    # Arguments\n        x : Tensor.\n        axis: Integer, axis along which the softmax normalization is applied.\n    # Returns\n        Tensor, output of softmax transformation.\n    # Raises\n        ValueError: In case `dim(x) == 1`.\n    \"\"\"\n    ndim = K.ndim(x)\n    if ndim == 2:\n        return K.softmax(x)\n    elif ndim > 2:\n        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n        s = K.sum(e, axis=axis, keepdims=True)\n        return e / s\n    else:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we are using the same hiden state of the post attention lstm with all the hidden states from all the units of the pre attention lstm thats why we need to repeat it \nrepeator = RepeatVector(15)\n#concatinate before passing to the densor to calculate the respective energie \nconcatenator = Concatenate(axis=-1)\n#calculate the energie \ndensor1 = Dense(10, activation = \"tanh\")\n#the final value for the energies (making sure they are +)\ndensor2 = Dense(1, activation = \"relu\")\n#calculates the attention weights\nactivator = Activation(softmax, name='attention_weights')\noutput_layer = Dense(10558, activation=softmax)\ndotor = Dot(axes = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_step_attention(a, s_prev):\n    #passing the previous hidden state \n    s_prev = repeator(s_prev) \n    #concatinate all the privous hidden states from the pre attention lstm with the current one \n    concat = concatenator([a,s_prev]) \n    #calculate the energie \n    e = densor1(concat)\n    #passing only the + values \n    energies = densor2(e) \n    #calculates the attention weights\n    alphas = activator(energies)\n    #calculates the context vector \n    context = dotor([alphas,a]) \n    return context","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_a = 4040\nn_s = 8080\nX = Input(shape=(15,4040))\ns0 = Input(shape=(n_s,), name='s0')\nc0 = Input(shape=(n_s,), name='c0')\ns = s0\nc = c0\noutputs = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=encoder(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t in range(15):\n    context = one_step_attention(a, s)\n    s, _, c = decoder(context,initial_state = [s, c] ) \n    out = output_layer(s)\n    outputs.append(out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=[X,s0,c0],outputs=outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999,decay=0.01) \nmodel.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s0 = np.zeros((1, n_s))\nc0 = np.zeros((1, n_s))\noutputs = list(oh_targ_seq.swapaxes(0,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit([oh_inp_seq, s0, c0], outputs, epochs=1, batch_size=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}